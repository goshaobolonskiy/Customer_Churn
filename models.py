"""
–ú–æ–¥—É–ª—å —Å –º–æ–¥–µ–ª—è–º–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è Customer Churn.
–°–æ–¥–µ—Ä–∂–∏—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π.
"""

import pandas as pd
import numpy as np
import pickle
import os
from datetime import datetime
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             f1_score, roc_auc_score, confusion_matrix,
                             classification_report)
import warnings

warnings.filterwarnings('ignore')


# ============================================
# 1. –ë–ê–ó–û–í–´–ï –ú–û–î–ï–õ–ò (SKLEARN)
# ============================================

def get_logistic_regression(**kwargs):
    """–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è"""
    from sklearn.linear_model import LogisticRegression

    params = {
        'C': 1.0,
        'max_iter': 1000,
        'random_state': 42,
        'class_weight': 'balanced',
        'n_jobs': -1
    }
    params.update(kwargs)

    return LogisticRegression(**params)


def get_random_forest(**kwargs):
    """–°–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å"""
    from sklearn.ensemble import RandomForestClassifier

    params = {
        'n_estimators': 100,
        'max_depth': 10,
        'min_samples_split': 5,
        'min_samples_leaf': 2,
        'random_state': 42,
        'class_weight': 'balanced',
        'n_jobs': -1
    }
    params.update(kwargs)

    return RandomForestClassifier(**params)


def get_gradient_boosting(**kwargs):
    """–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥ (sklearn)"""
    from sklearn.ensemble import GradientBoostingClassifier

    params = {
        'n_estimators': 100,
        'learning_rate': 0.1,
        'max_depth': 3,
        'random_state': 42
    }
    params.update(kwargs)

    return GradientBoostingClassifier(**params)


# ============================================
# 2. –ë–£–°–¢–ò–ù–ì–ò
# ============================================

def get_xgboost(**kwargs):
    """XGBoost"""
    from xgboost import XGBClassifier

    params = {
        'n_estimators': 100,
        'max_depth': 6,
        'learning_rate': 0.1,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'reg_alpha': 0.1,
        'reg_lambda': 1,
        'random_state': 42,
        'eval_metric': 'logloss',
        'use_label_encoder': False
    }
    params.update(kwargs)

    return XGBClassifier(**params)


def get_lightgbm(**kwargs):
    """LightGBM"""
    from lightgbm import LGBMClassifier

    params = {
        'n_estimators': 100,
        'max_depth': -1,
        'num_leaves': 31,
        'learning_rate': 0.1,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'reg_alpha': 0.1,
        'reg_lambda': 0.1,
        'random_state': 42,
        'verbose': -1
    }
    params.update(kwargs)

    return LGBMClassifier(**params)


def get_catboost(**kwargs):
    """CatBoost"""
    from catboost import CatBoostClassifier

    params = {
        'iterations': 100,
        'learning_rate': 0.1,
        'depth': 6,
        'l2_leaf_reg': 3,
        'border_count': 128,
        'random_seed': 42,
        'verbose': False,
        'auto_class_weights': 'Balanced'
    }
    params.update(kwargs)

    return CatBoostClassifier(**params)


# ============================================
# 3. –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô
# ============================================

def train_model(model, X_train, y_train, X_val=None, y_val=None, verbose=True):
    """
    –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π.

    Parameters:
    -----------
    model : sklearn-—Å–æ–≤–º–µ—Å—Ç–∏–º–∞—è –º–æ–¥–µ–ª—å
    X_train, y_train : –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
    X_val, y_val : –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    verbose : bool, –ø–µ—á–∞—Ç–∞—Ç—å –ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é

    Returns:
    --------
    model : –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
    history : dict —Å –∏—Å—Ç–æ—Ä–∏–µ–π –æ–±—É—á–µ–Ω–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å)
    """
    history = {}

    # –î–ª—è –º–æ–¥–µ–ª–µ–π —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π early_stopping
    if hasattr(model, 'fit') and 'eval_set' in model.fit.__code__.co_varnames:
        if X_val is not None and y_val is not None:
            model.fit(
                X_train, y_train,
                eval_set=[(X_val, y_val)],
                verbose=False
            )
            if hasattr(model, 'evals_result_'):
                history = model.evals_result_
        else:
            model.fit(X_train, y_train)
    else:
        model.fit(X_train, y_train)

    if verbose:
        print(f"  –ú–æ–¥–µ–ª—å {model.__class__.__name__} –æ–±—É—á–µ–Ω–∞")

    return model, history


# ============================================
# 4. –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ï–ô
# ============================================

def evaluate_model(model, X_test, y_test, verbose=True):
    """
    –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

    Returns:
    --------
    dict —Å–æ –≤—Å–µ–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
    """
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None

    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    metrics = {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred),
        'recall': recall_score(y_test, y_pred),
        'f1': f1_score(y_test, y_pred)
    }

    # ROC-AUC (–µ—Å–ª–∏ –µ—Å—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏)
    if y_proba is not None:
        try:
            metrics['roc_auc'] = roc_auc_score(y_test, y_proba)
        except:
            metrics['roc_auc'] = None

    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
    metrics['confusion_matrix'] = confusion_matrix(y_test, y_pred)

    if verbose:
        print(f"\nüìä –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ {model.__class__.__name__}:")
        print(f"  Accuracy:  {metrics['accuracy']:.4f}")
        print(f"  Precision: {metrics['precision']:.4f}")
        print(f"  Recall:    {metrics['recall']:.4f}")
        print(f"  F1-score:  {metrics['f1']:.4f}")
        if metrics.get('roc_auc'):
            print(f"  ROC-AUC:   {metrics['roc_auc']:.4f}")
        print(f"\n  –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫:")
        print(f"  {metrics['confusion_matrix']}")

    return metrics


# ============================================
# 5. –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô
# ============================================

def compare_models(models_dict, X_train, y_train, X_test, y_test, verbose=True):
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π.

    Parameters:
    -----------
    models_dict : dict
        –°–ª–æ–≤–∞—Ä—å –≤–∏–¥–∞ {'–Ω–∞–∑–≤–∞–Ω–∏–µ': –º–æ–¥–µ–ª—å}

    Returns:
    --------
    DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    """
    results = []

    for name, model in models_dict.items():
        if verbose:
            print(f"\n--- {name} ---")

        # –û–±—É—á–µ–Ω–∏–µ
        model, _ = train_model(model, X_train, y_train, verbose=verbose)

        # –û—Ü–µ–Ω–∫–∞
        metrics = evaluate_model(model, X_test, y_test, verbose=verbose)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results.append({
            'Model': name,
            'Accuracy': metrics['accuracy'],
            'Precision': metrics['precision'],
            'Recall': metrics['recall'],
            'F1': metrics['f1'],
            'ROC_AUC': metrics.get('roc_auc', None)
        })

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ F1
    results_df = pd.DataFrame(results).sort_values('F1', ascending=False)

    if verbose:
        print("\n" + "=" * 80)
        print("–°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô (–ø–æ —É–±—ã–≤–∞–Ω–∏—é F1)")
        print("=" * 80)
        print(results_df.to_string(index=False))

    return results_df


# ============================================
# 6. –ü–û–ò–°–ö –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò
# ============================================

def find_best_model(X_train, y_train, X_test, y_test,
                    models_to_try='all', verbose=True):
    """
    –ü–æ–∏—Å–∫ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –∏–∑ –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞.

    Parameters:
    -----------
    models_to_try : str or list
        'all' - –≤—Å–µ –º–æ–¥–µ–ª–∏
        'sklearn' - —Ç–æ–ª—å–∫–æ sklearn –º–æ–¥–µ–ª–∏
        'boosting' - —Ç–æ–ª—å–∫–æ –±—É—Å—Ç–∏–Ω–≥–∏
        list - —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π
    """

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–±–æ—Ä –º–æ–¥–µ–ª–µ–π
    all_models = {
        'LogisticRegression': get_logistic_regression(),
        'RandomForest': get_random_forest(),
        'GradientBoosting': get_gradient_boosting(),
        'XGBoost': get_xgboost(),
        'LightGBM': get_lightgbm(),
        'CatBoost': get_catboost()
    }

    if models_to_try == 'all':
        models = all_models
    elif models_to_try == 'sklearn':
        models = {k: v for k, v in all_models.items()
                  if k in ['LogisticRegression', 'RandomForest', 'GradientBoosting']}
    elif models_to_try == 'boosting':
        models = {k: v for k, v in all_models.items()
                  if k in ['XGBoost', 'LightGBM', 'CatBoost']}
    elif isinstance(models_to_try, list):
        models = {k: all_models[k] for k in models_to_try if k in all_models}
    else:
        models = all_models

    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –º–æ–¥–µ–ª–∏
    results = compare_models(models, X_train, y_train, X_test, y_test, verbose=verbose)

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
    best_model_name = results.iloc[0]['Model']
    best_model = all_models[best_model_name]

    if verbose:
        print(f"\nüèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_name} (F1 = {results.iloc[0]['F1']:.4f})")

    return best_model_name, best_model, results
